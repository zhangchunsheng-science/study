{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a459394",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "     reviews = f.readlines()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "     labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d13c1a0-4a27-4032-8986-7585bfae3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# we can get punctuation from string library\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039bfbfa-2641-404d-bc8d-948ca7ed5e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_reviews=list()\n",
    "for text in reviews:\n",
    "  text = text.lower()\n",
    "  text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "  all_reviews.append(text)\n",
    "all_text = \" \".join(all_reviews)\n",
    "all_words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56151e13-09c2-46e1-8806-dcb2f758bed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten occuring words : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 336713),\n",
       " ('and', 164107),\n",
       " ('a', 163009),\n",
       " ('of', 145864),\n",
       " ('to', 135720),\n",
       " ('is', 107328),\n",
       " ('br', 101872),\n",
       " ('it', 96352),\n",
       " ('in', 93968),\n",
       " ('i', 87623)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(all_words)\n",
    "total_words=len(all_words)\n",
    "sorted_words=count_words.most_common(total_words)\n",
    "print(\"Top ten occuring words : \")\n",
    "sorted_words[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89520c7d-8530-4566-880a-a88bd5203010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d921d2a-f152-46c5-ad5a-001708d67432",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews=list()\n",
    "for review in all_reviews:\n",
    "  encoded_review=list()\n",
    "  for word in review.split():\n",
    "    if word not in vocab_to_int.keys():\n",
    "      # if word is not available in vocab_to_int put 0 in that place\n",
    "      encoded_review.append(0)\n",
    "    else:\n",
    "      encoded_review.append(vocab_to_int[word])\n",
    "  encoded_reviews.append(encoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db83aa8-b44d-4949-aff8-74abe9c98869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length=250\n",
    "features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n",
    "for i, review in enumerate(encoded_reviews):\n",
    "  review_len=len(review)\n",
    "  if (review_len<=sequence_length):\n",
    "    zeros=list(np.zeros(sequence_length-review_len))\n",
    "    new=zeros+review\n",
    "  else:\n",
    "    new=review[:sequence_length]\n",
    "features[i,:]=np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861c58b0-894e-4b68-a4bd-5b9a73b97ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[1 if label.strip()=='positive' else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6044a11-9229-464d-97a7-7721e360f220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 2500 2500\n"
     ]
    }
   ],
   "source": [
    "train_x=features[:int(0.8*len(features))]\n",
    "train_y=labels[:int(0.8*len(features))]\n",
    "valid_x=features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "valid_y=labels[int(0.8*len(features)):int(0.9*len(features))]\n",
    "test_x=features[int(0.9*len(features)):]\n",
    "test_y=labels[int(0.9*len(features)):]\n",
    "print(len(train_y), len(valid_y), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a67ffbe-2ee7-488a-99f5-e69ec358959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create Tensor Dataset\n",
    "train_data=TensorDataset(torch.LongTensor(train_x), torch.LongTensor(train_y))\n",
    "valid_data=TensorDataset(torch.LongTensor(valid_x), torch.LongTensor(valid_y))\n",
    "test_data=TensorDataset(torch.LongTensor(test_x), torch.LongTensor(test_y))\n",
    "\n",
    "# dataloader\n",
    "batch_size=50\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702fe4a2-f4a8-4b36-8157-da2f78114a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    " \n",
    "class SentimentalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.hidden_dim=hidden_dim\n",
    "        \n",
    "        #Embedding and LSTM layers\n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        #Linear and sigmoid layer\n",
    "        self.fc1=nn.Linear(hidden_dim, 64)\n",
    "        self.fc2=nn.Linear(64, 16)\n",
    "        self.fc3=nn.Linear(16,output_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size=x.size()\n",
    "        \n",
    "        #Embadding and LSTM output\n",
    "        embedd=self.embedding(x)\n",
    "        lstm_out, hidden=self.lstm(embedd, hidden)\n",
    "        \n",
    "        #stack up the lstm output\n",
    "        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        #dropout and fully connected layers\n",
    "        out=self.dropout(lstm_out)\n",
    "        out=self.fc1(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc3(out)\n",
    "        sig_out=self.sigmoid(out)\n",
    "        \n",
    "        sig_out=sig_out.view(batch_size, -1)\n",
    "        sig_out=sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize Hidden STATE\"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1b311a-12b1-4144-8491-c1aa498af57a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentalLSTM(\n",
      "  (embedding): Embedding(74073, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5966da-a453-4591-b3fb-7deff8b850a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.681120... Val Loss: 0.694607\n",
      "Epoch: 1/3... Step: 200... Loss: 0.681912... Val Loss: 0.693378\n",
      "Epoch: 1/3... Step: 300... Loss: 0.686165... Val Loss: 0.693174\n",
      "Epoch: 1/3... Step: 400... Loss: 0.690441... Val Loss: 0.693300\n",
      "Epoch: 2/3... Step: 500... Loss: 0.696879... Val Loss: 0.693147\n",
      "Epoch: 2/3... Step: 600... Loss: 0.692132... Val Loss: 0.693195\n",
      "Epoch: 2/3... Step: 700... Loss: 0.690033... Val Loss: 0.693591\n",
      "Epoch: 2/3... Step: 800... Loss: 0.689303... Val Loss: 0.693196\n",
      "Epoch: 3/3... Step: 900... Loss: 0.695910... Val Loss: 0.693164\n",
      "Epoch: 3/3... Step: 1000... Loss: 0.696582... Val Loss: 0.693318\n",
      "Epoch: 3/3... Step: 1100... Loss: 0.689488... Val Loss: 0.693266\n",
      "Epoch: 3/3... Step: 1200... Loss: 0.694639... Val Loss: 0.693182\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "952c5370-09a1-4784-94e5-a2eea6b81e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.693\n",
      "Test accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    output, h = net(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
